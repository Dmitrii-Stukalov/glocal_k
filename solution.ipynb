{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 20000\n"
     ]
    }
   ],
   "source": [
    "train = np.loadtxt('ml-100k/u1.base', delimiter='\\t').astype('int32')\n",
    "test = np.loadtxt('ml-100k/u1.test', delimiter='\\t').astype('int32')\n",
    "total = np.concatenate((train, test), axis=0)\n",
    "\n",
    "n_u = np.unique(total[:, 0]).size  # num of users\n",
    "n_m = np.unique(total[:, 1]).size  # num of movies\n",
    "n_train = train.shape[0]  # num of training ratings\n",
    "n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "for i in range(n_train):\n",
    "    train_r[train[i, 1] - 1, train[i, 0] - 1] = train[i, 2]\n",
    "\n",
    "for i in range(n_test):\n",
    "    test_r[test[i, 1] - 1, test[i, 0] - 1] = test[i, 2]\n",
    "\n",
    "train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "print('data matrix loaded')\n",
    "print('num of users: {}'.format(n_u))\n",
    "print('num of movies: {}'.format(n_m))\n",
    "print('num of training ratings: {}'.format(n_train))\n",
    "print('num of test ratings: {}'.format(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500\n",
    "n_dim = 5\n",
    "n_layers = 2\n",
    "gk_size = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Hyperparameters to tune for specific case\n",
    "max_epoch_p = 501  # max number of epochs for pretraining\n",
    "max_epoch_f = 1001  # max number of epochs for finetuning\n",
    "patience_p = 5  # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
    "patience_f = 10  # and finetuning\n",
    "tol_p = 1e-4  # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
    "tol_f = 1e-5  # and finetuning\n",
    "lambda_2 = 20.  # regularisation of number or parameters\n",
    "lambda_s = 0.006  # regularisation of sparsity of the final matrix\n",
    "dot_scale = 1  # dot product weight for global kernel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def local_kernel(u, v):\n",
    "    dist = torch.norm(u - v, p=2, dim=2)\n",
    "    hat = torch.clamp(1. - dist ** 2, min=0.)\n",
    "    return hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class KernelLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
    "        self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
    "        self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
    "        self.b = nn.Parameter(torch.randn(n_hid))\n",
    "\n",
    "        self.lambda_s = lambda_s\n",
    "        self.lambda_2 = lambda_2\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.b)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_hat = local_kernel(self.u, self.v)\n",
    "\n",
    "        sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
    "        sparse_reg_term = self.lambda_s * sparse_reg\n",
    "\n",
    "        l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
    "        l2_reg_term = self.lambda_2 * l2_reg\n",
    "\n",
    "        W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
    "        y = torch.matmul(x, W_eff) + self.b\n",
    "        y = self.activation(y)\n",
    "\n",
    "        return y, sparse_reg_term + l2_reg_term"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
    "            else:\n",
    "                layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
    "        layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "\n",
    "    def forward(self, x):\n",
    "        total_reg = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, reg = layer(x)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.dropout(x)\n",
    "            if total_reg is None:\n",
    "                total_reg = reg\n",
    "            else:\n",
    "                total_reg += reg\n",
    "        return x, total_reg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class CompleteNet(nn.Module):\n",
    "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
    "        super().__init__()\n",
    "        self.gk_size = gk_size\n",
    "        self.dot_scale = dot_scale\n",
    "        self.local_kernel_net = kernel_net\n",
    "        self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size ** 2) * 0.1)\n",
    "        nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x, x_local):\n",
    "        gk = self.global_kernel(x_local, self.gk_size, self.dot_scale)\n",
    "        x = self.global_conv(x, gk)\n",
    "        x, global_reg_loss = self.local_kernel_net(x)\n",
    "        return x, global_reg_loss\n",
    "\n",
    "    def global_kernel(self, input, gk_size, dot_scale):\n",
    "        avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
    "        avg_pooling = avg_pooling.view(1, -1)\n",
    "\n",
    "        gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
    "        gk = gk.view(1, 1, gk_size, gk_size)\n",
    "\n",
    "        return gk\n",
    "\n",
    "    def global_conv(self, input, W):\n",
    "        input = input.unsqueeze(0).unsqueeze(0)\n",
    "        conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
    "        return conv2d.squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
    "        # L2 loss\n",
    "        diff = train_m * (train_r - pred_p)\n",
    "        sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
    "        loss_p = sqE + reg_loss\n",
    "        return loss_p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).float().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size,\n",
    "                             dot_scale).float().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE-TRAINING\n",
      "Epoch: 0 test rmse: 2.7551959 train rmse: 2.7292118\n",
      "Time: 0.7320170402526855 seconds\n",
      "Time cumulative: 0.7320170402526855 seconds\n",
      "PRE-TRAINING\n",
      "Epoch: 50 test rmse: 1.0251902 train rmse: 0.9841468\n",
      "Time: 2.8504528999328613 seconds\n",
      "Time cumulative: 90.85948324203491 seconds\n",
      "PRE-TRAINING\n",
      "Epoch: 100 test rmse: 0.963181 train rmse: 0.91872776\n",
      "Time: 4.979596853256226 seconds\n",
      "Time cumulative: 287.83330845832825 seconds\n",
      "PRE-TRAINING\n",
      "Epoch: 127 test rmse: 0.95846754 train rmse: 0.9138297\n",
      "Time: 6.0169031620025635 seconds\n",
      "Time cumulative: 431.11435651779175 seconds\n"
     ]
    }
   ],
   "source": [
    "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
    "best_rmse, best_mae, best_ndcg = float('inf'), float('inf'), 0\n",
    "\n",
    "time_cumulative = 0\n",
    "tic = time()\n",
    "\n",
    "# Pre-Training\n",
    "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    x = torch.Tensor(train_r).float().to(device)\n",
    "    m = torch.Tensor(train_m).float().to(device)\n",
    "    complete_model.local_kernel_net.train()\n",
    "    pred, reg = complete_model.local_kernel_net(x)\n",
    "    loss = Loss().to(device)(pred, reg, m, x)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(max_epoch_p):\n",
    "    optimizer.step(closure)\n",
    "    complete_model.local_kernel_net.eval()\n",
    "    t = time() - tic\n",
    "    time_cumulative += t\n",
    "\n",
    "    pre, _ = model(torch.Tensor(train_r).float().to(device))\n",
    "\n",
    "    pre = pre.float().cpu().detach().numpy()\n",
    "\n",
    "    error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "    test_rmse = np.sqrt(error)\n",
    "\n",
    "    error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "    train_rmse = np.sqrt(error_train)\n",
    "\n",
    "    if last_rmse - train_rmse < tol_p:\n",
    "        counter += 1\n",
    "    else:\n",
    "        counter = 0\n",
    "\n",
    "    last_rmse = train_rmse\n",
    "\n",
    "    if patience_p == counter:\n",
    "        print('PRE-TRAINING')\n",
    "        print('Epoch:', i + 1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    "        print('Time:', t, 'seconds')\n",
    "        print('Time cumulative:', time_cumulative, 'seconds')\n",
    "        break\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print('PRE-TRAINING')\n",
    "        print('Epoch:', i, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    "        print('Time:', t, 'seconds')\n",
    "        print('Time cumulative:', time_cumulative, 'seconds')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNING\n",
      "Epoch: 0 test rmse: 1.0568719 test mae: 0.85774463\n",
      "Epoch: 0 train rmse: 1.0274782 train mae: 0.8347582\n",
      "Time: 6.147149085998535 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 50 test rmse: 0.91829413 test mae: 0.7246843\n",
      "Epoch: 50 train rmse: 0.8555746 train mae: 0.6759926\n",
      "Time: 8.51752495765686 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 100 test rmse: 0.91115785 test mae: 0.71845055\n",
      "Epoch: 100 train rmse: 0.8490566 train mae: 0.67007416\n",
      "Time: 10.976081132888794 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 150 test rmse: 0.9085041 test mae: 0.7159648\n",
      "Epoch: 150 train rmse: 0.8445192 train mae: 0.6664536\n",
      "Time: 13.349500179290771 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 200 test rmse: 0.9089046 test mae: 0.71515864\n",
      "Epoch: 200 train rmse: 0.8410914 train mae: 0.66259533\n",
      "Time: 15.778004884719849 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 250 test rmse: 0.9091276 test mae: 0.71467775\n",
      "Epoch: 250 train rmse: 0.8394677 train mae: 0.66124177\n",
      "Time: 18.275057077407837 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 300 test rmse: 0.9089401 test mae: 0.7156781\n",
      "Epoch: 300 train rmse: 0.8379636 train mae: 0.6608618\n",
      "Time: 20.68306612968445 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 350 test rmse: 0.90840065 test mae: 0.7151788\n",
      "Epoch: 350 train rmse: 0.83736855 train mae: 0.66068274\n",
      "Time: 23.09041714668274 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 400 test rmse: 0.9087769 test mae: 0.71661556\n",
      "Epoch: 400 train rmse: 0.83671373 train mae: 0.660993\n",
      "Time: 25.57486891746521 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 450 test rmse: 0.91016036 test mae: 0.71967703\n",
      "Epoch: 450 train rmse: 0.8378014 train mae: 0.6636163\n",
      "Time: 27.949971914291382 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 500 test rmse: 0.90916824 test mae: 0.7157281\n",
      "Epoch: 500 train rmse: 0.8351451 train mae: 0.65896744\n",
      "Time: 30.370985984802246 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 550 test rmse: 0.90767926 test mae: 0.7147709\n",
      "Epoch: 550 train rmse: 0.8348571 train mae: 0.658549\n",
      "Time: 32.861642837524414 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 600 test rmse: 0.9084631 test mae: 0.71547127\n",
      "Epoch: 600 train rmse: 0.83406603 train mae: 0.6583001\n",
      "Time: 35.23067307472229 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 650 test rmse: 0.9075962 test mae: 0.71457773\n",
      "Epoch: 650 train rmse: 0.8331577 train mae: 0.65733874\n",
      "Time: 37.622036933898926 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 700 test rmse: 0.9086616 test mae: 0.71695095\n",
      "Epoch: 700 train rmse: 0.83238053 train mae: 0.6582828\n",
      "Time: 40.085083961486816 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 750 test rmse: 0.9070298 test mae: 0.7140751\n",
      "Epoch: 750 train rmse: 0.82965916 train mae: 0.65425646\n",
      "Time: 42.46989583969116 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 800 test rmse: 0.9060313 test mae: 0.71390986\n",
      "Epoch: 800 train rmse: 0.82774144 train mae: 0.6537687\n",
      "Time: 44.88994812965393 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 850 test rmse: 0.9064172 test mae: 0.71405226\n",
      "Epoch: 850 train rmse: 0.82564604 train mae: 0.6518459\n",
      "Time: 47.35257315635681 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 900 test rmse: 0.906896 test mae: 0.7146611\n",
      "Epoch: 900 train rmse: 0.8235984 train mae: 0.6500822\n",
      "Time: 49.72310709953308 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 950 test rmse: 0.90654176 test mae: 0.7132574\n",
      "Epoch: 950 train rmse: 0.82213527 train mae: 0.6486423\n",
      "Time: 52.21982502937317 seconds\n",
      "FINE-TUNING\n",
      "Epoch: 1000 test rmse: 0.90682584 test mae: 0.7124326\n",
      "Epoch: 1000 train rmse: 0.82163185 train mae: 0.6467921\n",
      "Time: 54.677687883377075 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning\n",
    "\n",
    "train_r_local = np.clip(pre, 1., 5.)\n",
    "\n",
    "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    x = torch.Tensor(train_r).float().to(device)\n",
    "    x_local = torch.Tensor(train_r_local).float().to(device)\n",
    "    m = torch.Tensor(train_m).float().to(device)\n",
    "    complete_model.train()\n",
    "    pred, reg = complete_model(x, x_local)\n",
    "    loss = Loss().to(device)(pred, reg, m, x)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(max_epoch_f):\n",
    "    optimizer.step(closure)\n",
    "    complete_model.eval()\n",
    "    t = time() - tic\n",
    "    time_cumulative += t\n",
    "\n",
    "    pre, _ = complete_model(torch.Tensor(train_r).float().to(device), torch.Tensor(train_r_local).float().to(device))\n",
    "\n",
    "    pre = pre.float().cpu().detach().numpy()\n",
    "\n",
    "    error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "    test_rmse = np.sqrt(error)\n",
    "\n",
    "    error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "    train_rmse = np.sqrt(error_train)\n",
    "\n",
    "    test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
    "    train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
    "\n",
    "    if test_rmse < best_rmse:\n",
    "        best_rmse = test_rmse\n",
    "        best_rmse_ep = i + 1\n",
    "\n",
    "    if test_mae < best_mae:\n",
    "        best_mae = test_mae\n",
    "        best_mae_ep = i + 1\n",
    "\n",
    "    if last_rmse - train_rmse < tol_f:\n",
    "        counter += 1\n",
    "    else:\n",
    "        counter = 0\n",
    "\n",
    "    last_rmse = train_rmse\n",
    "\n",
    "    if patience_f == counter:\n",
    "        print('FINE-TUNING')\n",
    "        print('Epoch:', i + 1, 'test rmse:', test_rmse, 'test mae:', test_mae)\n",
    "        print('Epoch:', i + 1, 'train rmse:', train_rmse, 'train mae:', train_mae)\n",
    "        print('Time:', t, 'seconds')\n",
    "        break\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print('FINE-TUNING')\n",
    "        print('Epoch:', i, 'test rmse:', test_rmse, 'test mae:', test_mae)\n",
    "        print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae)\n",
    "        print('Time:', t, 'seconds')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 881  best rmse: 0.9054057\n",
      "Epoch: 930  best mae: 0.712293\n"
     ]
    }
   ],
   "source": [
    "# Final result\n",
    "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
    "print('Epoch:', best_mae_ep, ' best mae:', best_mae)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
